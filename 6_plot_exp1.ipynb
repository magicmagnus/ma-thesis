{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Functions for Exp1\n",
    "\n",
    "def plot_tpr_per_attack(args, results_df):\n",
    "\n",
    "    results_df['set_fpr'].unique()[0] # set_fpr should be the same for all experiments, so we can just take the first value\n",
    "\n",
    "    # drop the no_attack case from the results_df\n",
    "    results_df = results_df[results_df['attack_name'] != 'no_attack']\n",
    "\n",
    "\n",
    "    attack_names = results_df['attack_name'].unique()\n",
    "    wm_methods = results_df['wm_method'].unique()\n",
    "    models = results_df['model_id'].unique()\n",
    "\n",
    "    # order the attacks and methods based on the order in name_mapping\n",
    "    attack_names = np.array(sorted(attack_names, key=lambda x: list(ATTACK_NAME_MAPPING.keys()).index(x)))\n",
    "    wm_methods = np.array(sorted(wm_methods, key=lambda x: list(METHODS_NAME_MAPPING.keys()).index(x)))\n",
    "    models = np.array(sorted(models, key=lambda x: list(MODEL_NAME_MAPPING.keys()).index(x)))\n",
    "\n",
    "    # for each attack (rows), plot all 4 WM methods in 4 sublpots (cols), all 2 models as lines\n",
    "\n",
    "    ncols = wm_methods.shape[0] + 1 # per method, Ã¼lus one for title\n",
    "    nrows = attack_names.shape[0] # for each attack\n",
    "    fs = 10\n",
    "    fs_xticks = 8\n",
    "    fs_yticks = 8\n",
    "    fs_title = 14\n",
    "    y_adj = 0.937\n",
    "    title_height_ratio = 0.15#0.65\n",
    "    height_correction = 0\n",
    "    title = ( \n",
    "        f'Performance of watermarking methods under different attacks\\n'\n",
    "        f'for dataset \"{args.prompt_dataset}\" for experiments in \\n'\n",
    "        f'{args.dataset_identifier}'\n",
    "    )\n",
    "\n",
    "    fig, gs, title_axes = setup_gridspec_figure(\n",
    "        nrows=nrows, ncols=ncols ,\n",
    "        fs=fs, title=title, fs_title=fs_title,\n",
    "        y_adj=y_adj, title_height_ratio=title_height_ratio,\n",
    "        sp_width=2, sp_height=1.75, height_correction=height_correction,\n",
    "    )\n",
    "\n",
    "    # # set the titles for each row, as the attack names\n",
    "    # for i, ax in enumerate(title_axes):\n",
    "    #     ax.text(0.5, 0.4, ATTACK_NAME_MAPPING[attack_names[i]]['name'], fontsize=fs_title, fontweight=\"bold\", ha=\"center\", va=\"center\")\n",
    "                      \n",
    "    handles, labels = [], []\n",
    "\n",
    "    # loop through all attacks (rows), and then per attack, loop through all WM methods\n",
    "    for i, attack_name in enumerate(attack_names): # rows\n",
    "        attack_df = results_df[results_df['attack_name'] == attack_name]\n",
    "        if attack_name not in ATTACK_NAME_MAPPING:\n",
    "            continue\n",
    "\n",
    "        axes = [fig.add_subplot(gs[2*i +1, j]) for j in range(ncols)]\n",
    "        for j, wm_method in enumerate(np.concatenate((wm_methods, [\"title\"]))): # columns\n",
    "            if wm_method == \"title\": # last column is title of the attack\n",
    "                axes[j].axis('off')\n",
    "                axes[j].text(0.1, 0.5, ATTACK_NAME_MAPPING[attack_name]['name'], fontsize=fs, fontweight=\"bold\", ha=\"left\", va=\"center\")\n",
    "                if i == 0:\n",
    "                    axes[j].set_title('Attacktype', fontsize=fs)\n",
    "            else:\n",
    "                wm_df = attack_df[attack_df['wm_method'] == wm_method]\n",
    "                \n",
    "                # Set axis direction based on attack type\n",
    "                if ATTACK_NAME_MAPPING[attack_name]['order'] == 'low-to-high':\n",
    "                    axes[j].invert_xaxis()\n",
    "                    \n",
    "                if i == 0:\n",
    "                    axes[j].set_title(METHODS_NAME_MAPPING[wm_method], fontsize=fs)\n",
    "                \n",
    "                axes[j].set_yticks(np.arange(0, 1.1, 0.25))\n",
    "                axes[j].set_yticklabels(np.arange(0, 1.1, 0.25), fontsize=fs_yticks)\n",
    "                axes[j].set_ylim([-0.1, 1.1])\n",
    "                axes[j].grid(True, linestyle='--', alpha=0.5)\n",
    "                # set top and right spines to invisible\n",
    "                axes[j].spines['top'].set_visible(False)\n",
    "                axes[j].spines['right'].set_visible(False)\n",
    "\n",
    "                if j == 0:# Add y-axis label to the first plot in each row\n",
    "                    axes[j].set_ylabel(\"TPR@FPR=0.01\")\n",
    "                else:# disable y-axis labels for all but the first column\n",
    "                    plt.setp(axes[j].get_yticklabels(), visible=False)\n",
    "                    plt.setp(axes[j].get_yticklines(), visible=False)\n",
    "\n",
    "                for model in models: # lines\n",
    "                    model_df = wm_df[wm_df['model_id'] == model]\n",
    "                    # Check if the model_df is empty\n",
    "                    if model_df.empty:\n",
    "                        print(f\"Warning: No data for {attack_name}, {wm_method}, {model}\\n\")\n",
    "                        continue\n",
    "\n",
    "                    if attack_name == 'no_attack':\n",
    "                        # No need to order the attack strengths for the no attack case\n",
    "                        strengths = model_df['attack_strength'].unique()\n",
    "                        results = model_df['tpr_empirical'].values\n",
    "                        ci_lower = model_df['tpr_ci_lower_percentile'].values\n",
    "                        ci_upper = model_df['tpr_ci_upper_percentile'].values\n",
    "                    else:\n",
    "                        strengths, results, ci_lower, ci_upper = order_attack_strengths(\n",
    "                            ATTACK_NAME_MAPPING[attack_name]['order'],\n",
    "                            model_df['attack_strength'], \n",
    "                            model_df['tpr_empirical'],\n",
    "                            model_df['tpr_ci_lower_percentile'],\n",
    "                            model_df['tpr_ci_upper_percentile'],\n",
    "                            ATTACK_NAME_MAPPING[attack_name]['cast_to_int'],\n",
    "                        )\n",
    "                    \n",
    "                    label = MODEL_NAME_MAPPING[model]['name']\n",
    "                    \n",
    "                    # Plot using actual strength values\n",
    "                    line, = axes[j].plot(strengths, results,\n",
    "                                marker=MODEL_NAME_MAPPING[model]['marker'],\n",
    "                                linestyle=MODEL_NAME_MAPPING[model]['line'],\n",
    "                                label=label,\n",
    "                                color=MODEL_NAME_MAPPING[model]['color'])\n",
    "                    \n",
    "                    if (not np.isnan(ci_lower).any() and not np.isnan(ci_upper).any()) or (len(ci_lower) > 0 and len(ci_upper) > 0):\n",
    "                        axes[j].fill_between(strengths, ci_lower, ci_upper, color=MODEL_NAME_MAPPING[model]['color'], alpha=0.2)\n",
    "                        if attack_name == 'no_attack':\n",
    "                            axes[j].plot(strengths, ci_lower, color=MODEL_NAME_MAPPING[model]['color'], alpha=0.2, marker='x', linestyle='--')\n",
    "                            axes[j].plot(strengths, ci_upper, color=MODEL_NAME_MAPPING[model]['color'], alpha=0.2, marker='x', linestyle='--')\n",
    "\n",
    "                                \n",
    "                    if label not in labels:\n",
    "                        handles.append(line)\n",
    "                        labels.append(label)\n",
    "\n",
    "                    # Set only the actual strength values as ticks\n",
    "                    axes[j].set_xticks(strengths)\n",
    "                    axes[j].set_xticklabels(strengths, fontsize=fs_xticks)\n",
    "                    #axes[j].set_xlim([strengths[0]-0.1, strengths[-1]+0.1])\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.2, 0.4, 0.5, 0.5), ncol=len(models), handles=handles, labels=labels)\n",
    "    \n",
    "\n",
    "    plt.savefig(args.output_plot, bbox_inches='tight', dpi=300)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {args.output_plot}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tpr_per_metric(args, results_df, metric_name, metric_column, title_suffix, xlabel, xlim):\n",
    "    \"\"\"\n",
    "    Generic plotting function that can use any metric for the x-axis\n",
    "    \n",
    "    Parameters:\n",
    "    - args: The command line arguments\n",
    "    - metric_name: String identifier for the metric (used in filenames)\n",
    "    - metric_column: Name of the column to use for x-axis values\n",
    "    - title_suffix: Text to add to the plot title\n",
    "    - xlabel: Label for the x-axis\n",
    "    \"\"\"\n",
    "    # results_df = pd.read_csv(args.output_csv)\n",
    "    \n",
    "    attack_names = results_df['attack_name'].unique()\n",
    "    wm_methods = results_df['wm_method'].unique()\n",
    "    models = results_df['model_id'].unique()\n",
    "\n",
    "    attack_names = np.array(sorted(attack_names, key=lambda x: list(ATTACK_NAME_MAPPING.keys()).index(x)))\n",
    "    wm_methods = np.array(sorted(wm_methods, key=lambda x: list(METHODS_NAME_MAPPING.keys()).index(x)))\n",
    "    models = np.array(sorted(models, key=lambda x: list(MODEL_NAME_MAPPING.keys()).index(x)))\n",
    "\n",
    "    # Setup figure with same layout\n",
    "    ncols = wm_methods.shape[0] + 1 # per method, plus one for title\n",
    "    nrows = attack_names.shape[0]  # for each attack\n",
    "    fs = 10\n",
    "    fs_xticks = 8\n",
    "    fs_yticks = 8\n",
    "    fs_title = 14\n",
    "    y_adj = 0.937\n",
    "    title_height_ratio = 0.15\n",
    "    title = (\n",
    "        f'Watermarking performance vs {title_suffix}\\n'\n",
    "        f'for dataset \"{args.prompt_dataset}\" for experiments in \\n'\n",
    "        f'{args.dataset_identifier}'\n",
    "    )\n",
    "\n",
    "    fig, gs, title_axes = setup_gridspec_figure(\n",
    "        nrows=nrows, ncols=ncols,\n",
    "        fs=fs, title=title, fs_title=fs_title,\n",
    "        y_adj=y_adj, title_height_ratio=title_height_ratio,\n",
    "        sp_width=2, sp_height=1.75\n",
    "    )\n",
    "\n",
    "    # # Set row titles (attack names)\n",
    "    # for i, ax in enumerate(title_axes):\n",
    "    #     ax.text(0.5, 0.25, ATTACK_NAME_MAPPING[attack_names[i]]['name'], \n",
    "    #             fontsize=fs_title, fontweight=\"bold\", ha=\"center\", va=\"center\")\n",
    "                      \n",
    "    handles, labels = [], []\n",
    "    xticks_num = 3\n",
    "    xticks_stepsize = (xlim[1] - xlim[0]) / xticks_num\n",
    "    xticks = np.round(np.arange(xlim[0], xlim[1] + xticks_stepsize, xticks_stepsize), 2)\n",
    "    xlim_buffer = np.abs(xlim[1] - xlim[0]) * 0.07\n",
    "    xlim = (xlim[0] - xlim_buffer, xlim[1] + xlim_buffer)\n",
    "\n",
    "    # Loop through attacks and watermarking methods\n",
    "    for i, attack_name in enumerate(attack_names): # rows\n",
    "        attack_df = results_df[results_df['attack_name'] == attack_name]\n",
    "        if attack_name not in ATTACK_NAME_MAPPING:\n",
    "            continue\n",
    "\n",
    "        axes = [fig.add_subplot(gs[2*i +1, j]) for j in range(ncols)]\n",
    "        for j, wm_method in enumerate(np.concatenate((wm_methods, [\"title\"]))): # columns\n",
    "            if wm_method == \"title\": # last column is title of the attack\n",
    "                axes[j].axis('off')\n",
    "                axes[j].text(0.1, 0.5, ATTACK_NAME_MAPPING[attack_name]['name'], fontsize=fs, fontweight=\"bold\", ha=\"left\", va=\"center\")\n",
    "                if i == 0:\n",
    "                    axes[j].set_title('Attacktype', fontsize=fs)\n",
    "            else:\n",
    "                wm_df = attack_df[attack_df['wm_method'] == wm_method]\n",
    "\n",
    "                # Set axis direction based on attack type\n",
    "                if ATTACK_NAME_MAPPING[attack_name]['order'] == 'low-to-high':\n",
    "                    axes[j].invert_xaxis()\n",
    "                    \n",
    "                if i == 0:\n",
    "                    axes[j].set_title(METHODS_NAME_MAPPING[wm_method], fontsize=fs)\n",
    "                \n",
    "                axes[j].set_ylim([-0.1, 1.1])\n",
    "                axes[j].set_yticks(np.arange(0, 1.1, 0.25))\n",
    "                axes[j].set_yticklabels(np.arange(0, 1.1, 0.25), fontsize=fs_yticks)\n",
    "                axes[j].set_xlim(xlim)\n",
    "                axes[j].set_xticks(xticks)\n",
    "                axes[j].set_xticklabels(xticks, fontsize=fs_xticks)\n",
    "                axes[j].grid(True, linestyle='--', alpha=0.5)\n",
    "                # set top and right spines to invisible\n",
    "                axes[j].spines['top'].set_visible(False)\n",
    "                axes[j].spines['right'].set_visible(False)\n",
    "\n",
    "                if j == 0:# Add y-axis label to the first plot in each row\n",
    "                    axes[j].set_ylabel(\"TPR@FPR=0.01\")\n",
    "                else:# disable y-axis labels for all but the first column\n",
    "                    plt.setp(axes[j].get_yticklabels(), visible=False)\n",
    "                    plt.setp(axes[j].get_yticklines(), visible=False)\n",
    "                \n",
    "                # For quality metrics (like CLIP similarity score), higher is better, \n",
    "                # so have higher values to the left\n",
    "                if \"score\" in metric_column.lower() or \"similarity\" in metric_column.lower():\n",
    "                    #print(f\"enter score for {metric_column}\")\n",
    "                    if axes[j].get_xlim()[0] < axes[j].get_xlim()[1]:  # If lower values are on left\n",
    "                        #print(f\"enter score for {metric_column} invert\")\n",
    "                        axes[j].invert_xaxis()  # Invert so higher values are on left\n",
    "                # For distance metrics (like FID), lower is better, so have lower values to the left\n",
    "                if \"fid\" in metric_column.lower() or \"distance\" in metric_column.lower():\n",
    "                    #print(f\"enter fid for {metric_column}\")\n",
    "                    if axes[j].get_xlim()[0] > axes[j].get_xlim()[1]:  # If higher values are on left\n",
    "                        #print(f\"enter fid for {metric_column} invert\")\n",
    "                        axes[j].invert_xaxis()  # Invert so lower values are on left\n",
    "\n",
    "                for model in models: # lines\n",
    "                    model_df = wm_df[wm_df['model_id'] == model]\n",
    "                    \n",
    "                    # Check if the metric column exists\n",
    "                    if metric_column not in model_df.columns:\n",
    "                        print(f\"Warning: {metric_column} not found for {attack_name}, {wm_method}, {model}\")\n",
    "                        continue\n",
    "\n",
    "                    # Sort by the metric column\n",
    "                    df_sorted = model_df.sort_values(by=metric_column)\n",
    "                    x_values = df_sorted[metric_column].values\n",
    "                    tpr_values = df_sorted['tpr_empirical'].values\n",
    "                    attack_strengths = df_sorted['attack_strength'].values\n",
    "\n",
    "                    label = MODEL_NAME_MAPPING[model]['name']\n",
    "                    \n",
    "                    line, = axes[j].plot(x_values, tpr_values,\n",
    "                                marker=MODEL_NAME_MAPPING[model]['marker'],\n",
    "                                linestyle=MODEL_NAME_MAPPING[model]['line'],\n",
    "                                label=label,\n",
    "                                color=MODEL_NAME_MAPPING[model]['color'])\n",
    "                    \n",
    "                    # Add attack strength as text near each point for reference\n",
    "                    for k, (x, y, strength) in enumerate(zip(x_values, tpr_values, attack_strengths)):\n",
    "                        if k % 2 == 0:  # Only label every other point to avoid clutter\n",
    "                            axes[j].annotate(f\"{strength}\", (x, y), \n",
    "                                            textcoords=\"offset points\", \n",
    "                                            xytext=(0, 5), \n",
    "                                            ha='center',\n",
    "                                            fontsize=7)\n",
    "                                \n",
    "                    if label not in labels:\n",
    "                        handles.append(line)\n",
    "                        labels.append(label)\n",
    "\n",
    "    \n",
    "                \n",
    "                \n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.2, 0.4, 0.5, 0.5), ncol=len(models), handles=handles, labels=labels)\n",
    "    \n",
    "    output_plot = args.output_plot.replace('.pdf', f'_{metric_name}.pdf')\n",
    "    plt.savefig(output_plot, bbox_inches='tight', dpi=300)\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    print(f\"\\n{title_suffix} plot saved to {output_plot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLIP similarity score plot saved to experiments/exp1/_results/coco/num_200_fpr_0.01_cfg_3.0_wmch_16/num_200_fpr_0.01_cfg_3.0_wmch_16_plot_clip_score.pdf\n",
      "\n",
      "Abs. Mean Difference (originial - recovered) plot saved to experiments/exp1/_results/coco/num_200_fpr_0.01_cfg_3.0_wmch_16/num_200_fpr_0.01_cfg_3.0_wmch_16_plot_wm_diff.pdf\n",
      "\n",
      "FID (WM vs COCO) plot saved to experiments/exp1/_results/coco/num_200_fpr_0.01_cfg_3.0_wmch_16/num_200_fpr_0.01_cfg_3.0_wmch_16_plot_fid_coco.pdf\n",
      "\n",
      "FID (WM vs NOWM) plot saved to experiments/exp1/_results/coco/num_200_fpr_0.01_cfg_3.0_wmch_16/num_200_fpr_0.01_cfg_3.0_wmch_16_plot_fid_wm_nowm.pdf\n"
     ]
    }
   ],
   "source": [
    "args = Namespace()\n",
    "args.exp_name = 'exp1'\n",
    "\n",
    "\n",
    "# specify which experimental setup we want to plot\n",
    "args.num_imgs = 200\n",
    "args.prompt_dataset = 'coco'\n",
    "\n",
    "# for exp1, we merge results over wmch_16 for Flux and wmch_4 for SD\n",
    "args.dataset_identifier = [f'num_{args.num_imgs}_fpr_0.01_cfg_3.0_wmch_16', \n",
    "                           f'num_{args.num_imgs}_fpr_0.01_cfg_3.0_wmch_4'] \n",
    "\n",
    "\n",
    "# create the output directories and ffilenames\n",
    "args.input_dir = os.path.join('experiments', args.exp_name)\n",
    "args.output_dir = os.path.join('experiments', args.exp_name, '_results', args.prompt_dataset,  args.dataset_identifier[0])\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "args.output_plot = os.path.join(args.output_dir, args.dataset_identifier[0] + '_plot.pdf')\n",
    "args.merged_result_csv = os.path.join(args.output_dir, args.dataset_identifier[0] + '_merged.csv')\n",
    "\n",
    "# merged results already created in 5_merge_results.py\n",
    "results_df = pd.read_csv(args.merged_result_csv)\n",
    "\n",
    "# 1. plot TPR vs attack strength\n",
    "#plot_tpr_per_attack(args, results_df)\n",
    "\n",
    "# 2. plot TPR vs CLIP \n",
    "xmin = results_df['clip_score_wm'].min()\n",
    "xmax = results_df['clip_score_wm'].max()\n",
    "plot_tpr_per_metric(\n",
    "    args, \n",
    "    results_df, \n",
    "    metric_name=\"clip_score\", \n",
    "    metric_column=\"clip_score_wm\",\n",
    "    title_suffix=\"CLIP similarity score\",\n",
    "    xlabel=\"CLIP score (â)\",\n",
    "    xlim=[xmin, xmax]\n",
    ")\n",
    "\n",
    "# 3. plot TPR vs diff \n",
    "xmin = results_df['wm_diff'].min()\n",
    "xmax = results_df['wm_diff'].max()\n",
    "plot_tpr_per_metric(\n",
    "    args, \n",
    "    results_df, \n",
    "    metric_name=\"wm_diff\", \n",
    "    metric_column=\"wm_diff\",\n",
    "    title_suffix=\"Abs. Mean Difference (originial - recovered)\",\n",
    "    xlabel=\"Diff (â)\",\n",
    "    xlim=[xmin, xmax]\n",
    ")\n",
    "\n",
    "# 4. plot TPR vs FID (WM vs COCO)\n",
    "xmin = results_df['fid_wm_coco'].min()\n",
    "xmax = results_df['fid_wm_coco'].max()\n",
    "plot_tpr_per_metric(\n",
    "    args, \n",
    "    results_df, \n",
    "    metric_name=\"fid_coco\", \n",
    "    metric_column=\"fid_wm_coco\",\n",
    "    title_suffix=\"FID (WM vs COCO)\",\n",
    "    xlabel=\"FID (â)\",\n",
    "    xlim=[xmin, xmax]\n",
    ")\n",
    "\n",
    "# 5. plot TPR vs FID (WM vs NOWM)\n",
    "xmin = results_df['fid_wm_nowm'].min()\n",
    "xmax = results_df['fid_wm_nowm'].max()\n",
    "plot_tpr_per_metric(\n",
    "    args, \n",
    "    results_df, \n",
    "    metric_name=\"fid_wm_nowm\", \n",
    "    metric_column=\"fid_wm_nowm\",\n",
    "    title_suffix=\"FID (WM vs NOWM)\",\n",
    "    xlabel=\"FID (â)\",\n",
    "    xlim=[xmin, xmax]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
